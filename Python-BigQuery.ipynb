{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "client = bigquery.Client(location=\"US\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "query = \"\"\"\n",
    "    SELECT name, SUM(number) as total\n",
    "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "    GROUP BY name\n",
    "    ORDER BY total DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a name for the new dataset.\n",
    "dataset_id = 'pythonresearch'\n",
    "\n",
    "# The project defaults to the Client's project if not specified.\n",
    "dataset = client.create_dataset(dataset_id)  # API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write query results to a destination table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT corpus\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    GROUP BY corpus;\n",
    "\"\"\"\n",
    "table_ref = dataset.table(\"results\")\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    destination=table_ref\n",
    ")\n",
    "\n",
    "# Start the query, passing in the extra configuration.\n",
    "query_job = client.query(sql, location=\"US\", job_config=job_config)\n",
    "\n",
    "query_job.result()  # Waits for the query to finish\n",
    "print(\"Query results loaded to table {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a pandas DataFrame to a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"title\": \"The Meaning of Life\", \"release_year\": 1983},\n",
    "    {\"title\": \"Monty Python and the Holy Grail\", \"release_year\": 1975},\n",
    "    {\"title\": \"Life of Brian\", \"release_year\": 1979},\n",
    "    {\"title\": \"And Now for Something Completely Different\", \"release_year\": 1971},\n",
    "]\n",
    "\n",
    "# Optionally set explicit indices.\n",
    "# If indices are not specified, a column will be created for the default\n",
    "# indices created by pandas.\n",
    "index = [\"Q24980\", \"Q25043\", \"Q24953\", \"Q16403\"]\n",
    "df = pandas.DataFrame(records, index=pandas.Index(index, name=\"wikidata_id\"))\n",
    "\n",
    "table_ref = dataset.table(\"monty_python\")\n",
    "job = client.load_table_from_dataframe(df, table_ref, location=\"US\")\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\"Loaded dataframe to {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a local file to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filename = 'resources/us-states.csv'\n",
    "\n",
    "table_ref = dataset.table('us_states_from_local_file')\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "with open(source_filename, 'rb') as source_file:\n",
    "    job = client.load_table_from_file(\n",
    "        source_file,\n",
    "        table_ref,\n",
    "        location='US',  # Must match the destination dataset location.\n",
    "        job_config=job_config)  # API request\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print('Loaded {} rows into {}:{}.'.format(\n",
    "    job.output_rows, dataset_id, table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Cloud Storage to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the load job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField('name', 'STRING'),\n",
    "        bigquery.SchemaField('post_abbr', 'STRING')\n",
    "    ],\n",
    "    skip_leading_rows=1,\n",
    "    # The source format defaults to CSV. The line below is optional.\n",
    "    source_format=bigquery.SourceFormat.CSV\n",
    ")\n",
    "uri = 'gs://cloud-samples-data/bigquery/us-states/us-states.csv'\n",
    "destination_table_ref = dataset.table('us_states_from_gcs')\n",
    "\n",
    "# Start the load job\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, destination_table_ref, job_config=job_config)\n",
    "print('Starting job {}'.format(load_job.job_id))\n",
    "\n",
    "load_job.result()  # Waits for table load to complete.\n",
    "print('Job finished.')\n",
    "\n",
    "# Retreive the destination table\n",
    "destination_table = client.get_table(table_ref)\n",
    "print('Loaded {} rows.'.format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dataset from the API\n",
    "dataset = client.get_dataset(client.dataset(dataset_id))\n",
    "\n",
    "# Delete the dataset and its contents\n",
    "client.delete_dataset(dataset, delete_contents=True)\n",
    "\n",
    "print('Deleted dataset: {}'.format(dataset.path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
